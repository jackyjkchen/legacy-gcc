diff --git a/calls.c b/calls.c
index bec7f34..7290541 100644
--- a/calls.c
+++ b/calls.c
@@ -32,6 +32,10 @@ Boston, MA 02111-1307, USA.  */
 #endif
 #include "insn-flags.h"
 
+#if !defined PREFERRED_STACK_BOUNDARY && defined STACK_BOUNDARY
+#define PREFERRED_STACK_BOUNDARY STACK_BOUNDARY
+#endif
+
 /* Decide whether a function's arguments should be processed
    from first to last or from last to first.
 
@@ -46,8 +50,8 @@ Boston, MA 02111-1307, USA.  */
 
 #endif
 
-/* Like STACK_BOUNDARY but in units of bytes, not bits.  */
-#define STACK_BYTES (STACK_BOUNDARY / BITS_PER_UNIT)
+/* Like PREFERRED_STACK_BOUNDARY but in units of bytes, not bits.  */
+#define STACK_BYTES (PREFERRED_STACK_BOUNDARY / BITS_PER_UNIT)
 
 /* Data structure and subroutines used within expand_call.  */
 
@@ -314,7 +318,7 @@ prepare_call_address (funexp, fndecl, call_fusage, reg_parm_seen)
    says that the pointer to this aggregate is to be popped by the callee.
 
    STACK_SIZE is the number of bytes of arguments on the stack,
-   rounded up to STACK_BOUNDARY; zero if the size is variable.
+   rounded up to PREFERRED_STACK_BOUNDARY; zero if the size is variable.
    This is both to put into the call insn and
    to generate explicit popping code if necessary.
 
@@ -1223,8 +1227,8 @@ expand_call (exp, target, ignore)
       args_size.var = ARGS_SIZE_TREE (args_size);
       args_size.constant = 0;
 
-#ifdef STACK_BOUNDARY
-      if (STACK_BOUNDARY != BITS_PER_UNIT)
+#ifdef PREFERRED_STACK_BOUNDARY 
+      if (PREFERRED_STACK_BOUNDARY != BITS_PER_UNIT)
 	args_size.var = round_up (args_size.var, STACK_BYTES);
 #endif
 
@@ -1247,7 +1251,7 @@ expand_call (exp, target, ignore)
     }
   else
     {
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
       args_size.constant = (((args_size.constant + (STACK_BYTES - 1))
 			     / STACK_BYTES) * STACK_BYTES);
 #endif
@@ -1574,7 +1578,7 @@ expand_call (exp, target, ignore)
     }
 					       
 #ifdef PUSH_ARGS_REVERSED
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
   /* If we push args individually in reverse order, perform stack alignment
      before the first push (the last arg).  */
   if (argblock == 0)
@@ -1828,7 +1832,7 @@ expand_call (exp, target, ignore)
 		       args_size.var != 0, fndecl, reg_parm_stack_space);
 
 #ifndef PUSH_ARGS_REVERSED
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
   /* If we pushed args in forward order, perform stack alignment
      after pushing the last arg.  */
   if (argblock == 0)
@@ -2431,7 +2435,7 @@ emit_library_call VPROTO((rtx orgfun, int no_queue, enum machine_mode outmode,
   assemble_external_libcall (fun);
 
   original_args_size = args_size;
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
   args_size.constant = (((args_size.constant + (STACK_BYTES - 1))
 			 / STACK_BYTES) * STACK_BYTES);
 #endif
@@ -2499,7 +2503,7 @@ emit_library_call VPROTO((rtx orgfun, int no_queue, enum machine_mode outmode,
 #endif
 
 #ifdef PUSH_ARGS_REVERSED
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
   /* If we push args individually in reverse order, perform stack alignment
      before the first push (the last arg).  */
   if (argblock == 0)
@@ -2644,7 +2648,7 @@ emit_library_call VPROTO((rtx orgfun, int no_queue, enum machine_mode outmode,
     }
 
 #ifndef PUSH_ARGS_REVERSED
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
   /* If we pushed args in forward order, perform stack alignment
      after pushing the last arg.  */
   if (argblock == 0)
@@ -2999,7 +3003,7 @@ emit_library_call_value VPROTO((rtx orgfun, rtx value, int no_queue,
   assemble_external_libcall (fun);
 
   original_args_size = args_size;
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
   args_size.constant = (((args_size.constant + (STACK_BYTES - 1))
 			 / STACK_BYTES) * STACK_BYTES);
 #endif
@@ -3067,7 +3071,7 @@ emit_library_call_value VPROTO((rtx orgfun, rtx value, int no_queue,
 #endif
 
 #ifdef PUSH_ARGS_REVERSED
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
   /* If we push args individually in reverse order, perform stack alignment
      before the first push (the last arg).  */
   if (argblock == 0)
@@ -3212,7 +3216,7 @@ emit_library_call_value VPROTO((rtx orgfun, rtx value, int no_queue,
     }
 
 #ifndef PUSH_ARGS_REVERSED
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
   /* If we pushed args in forward order, perform stack alignment
      after pushing the last arg.  */
   if (argblock == 0)
diff --git a/config/i386/i386.c b/config/i386/i386.c
index a4ec3b6..a97b232 100644
--- a/config/i386/i386.c
+++ b/config/i386/i386.c
@@ -1995,6 +1995,68 @@ load_pic_register (do_rtl)
     emit_insn (gen_blockage ());
 }
 
+/* Compute the size of local storage taking into consideration the
+   desired stack alignment which is to be maintained.  Also determine
+   the number of registers saved below the local storage.  */
+
+HOST_WIDE_INT
+ix86_compute_frame_size (size, nregs_on_stack)
+     HOST_WIDE_INT size;
+     int *nregs_on_stack;
+{
+  int limit;
+  int nregs;
+  int regno;
+  int padding;
+  int pic_reg_used = flag_pic && (current_function_uses_pic_offset_table
+				  || current_function_uses_const_pool);
+  HOST_WIDE_INT total_size;
+
+  limit = frame_pointer_needed
+	  ? FRAME_POINTER_REGNUM : STACK_POINTER_REGNUM;
+
+  nregs = 0;
+
+  for (regno = limit - 1; regno >= 0; regno--)
+    if ((regs_ever_live[regno] && ! call_used_regs[regno])
+	|| (regno == PIC_OFFSET_TABLE_REGNUM && pic_reg_used))
+      nregs++;
+
+  padding = 0;
+  total_size = size + (nregs * UNITS_PER_WORD);
+
+#ifdef PREFERRED_STACK_BOUNDARY
+  {
+    int offset;
+    int preferred_alignment = PREFERRED_STACK_BOUNDARY / BITS_PER_UNIT;
+
+    offset = 4;
+    if (frame_pointer_needed)
+      offset += UNITS_PER_WORD;
+
+    total_size += offset;
+
+    padding = ((total_size + preferred_alignment - 1)
+	       & -preferred_alignment) - total_size;
+
+    if (padding < (((offset + preferred_alignment - 1)
+		    & -preferred_alignment) - offset))
+      padding += preferred_alignment;
+
+    /* Don't bother aligning the stack of a leaf function
+       which doesn't allocate any stack slots.  */
+    if (size == 0 && current_function_is_leaf)
+      padding = 0;
+  }
+#endif
+
+  if (nregs_on_stack)
+    *nregs_on_stack = nregs;
+
+  return size + padding;
+}
+
+
 static void
 ix86_prologue (do_rtl)
      int do_rtl;
@@ -2004,7 +2066,7 @@ ix86_prologue (do_rtl)
   rtx xops[4];
   int pic_reg_used = flag_pic && (current_function_uses_pic_offset_table
 				  || current_function_uses_const_pool);
-  long tsize = get_frame_size ();
+  HOST_WIDE_INT tsize = ix86_compute_frame_size (get_frame_size (), (int *)0);
   rtx insn;
   int cfa_offset = INCOMING_FRAME_SP_OFFSET, cfa_store_offset = cfa_offset;
   
@@ -2218,7 +2280,7 @@ ix86_epilogue (do_rtl)
   rtx xops[3];
   int pic_reg_used = flag_pic && (current_function_uses_pic_offset_table
 				  || current_function_uses_const_pool);
-  long tsize = get_frame_size ();
+  HOST_WIDE_INT tsize = ix86_compute_frame_size (get_frame_size (), &nregs);
 
   /* Compute the number of registers to pop */
 
diff --git a/config/i386/i386.h b/config/i386/i386.h
index bd90ab1..a9e0649 100644
--- a/config/i386/i386.h
+++ b/config/i386/i386.h
@@ -405,6 +405,11 @@ extern int ix86_arch;
 /* Boundary (in *bits*) on which stack pointer should be aligned.  */
 #define STACK_BOUNDARY 32
 
+/* We want to keep the stack aligned to 128 bits when possible, for the
+   benefit of doubles and SSE __m128.  But the compiler can not rely on
+   the stack having this alignment.*/
+#define PREFERRED_STACK_BOUNDARY 128
+
 /* Allocation boundary (in *bits*) for the code of a function.
    For i486, we get better performance by aligning to a cache
    line (i.e. 16 byte) boundary.  */
diff --git a/explow.c b/explow.c
index a2519f8..72bb9b6 100644
--- a/explow.c
+++ b/explow.c
@@ -31,6 +31,10 @@ Boston, MA 02111-1307, USA.  */
 #include "insn-flags.h"
 #include "insn-codes.h"
 
+#if !defined PREFERRED_STACK_BOUNDARY && defined STACK_BOUNDARY
+#define PREFERRED_STACK_BOUNDARY STACK_BOUNDARY
+#endif
+
 static rtx break_out_memory_refs	PROTO((rtx));
 static void emit_stack_probe		PROTO((rtx));
 /* Return an rtx for the sum of X and the integer C.
@@ -827,8 +831,8 @@ rtx
 round_push (size)
      rtx size;
 {
-#ifdef STACK_BOUNDARY
-  int align = STACK_BOUNDARY / BITS_PER_UNIT;
+#ifdef PREFERRED_STACK_BOUNDARY
+  int align = PREFERRED_STACK_BOUNDARY / BITS_PER_UNIT;
   if (align == 1)
     return size;
   if (GET_CODE (size) == CONST_INT)
@@ -848,7 +852,7 @@ round_push (size)
 			    NULL_RTX, 1);
       size = expand_mult (Pmode, size, GEN_INT (align), NULL_RTX, 1);
     }
-#endif /* STACK_BOUNDARY */
+#endif /* PREFERRED_STACK_BOUNDARY */
   return size;
 }
 
@@ -1047,10 +1051,10 @@ allocate_dynamic_stack_space (size, target, known_align)
      If we have to align, we must leave space in SIZE for the hole
      that might result from the alignment operation.  */
 
-#if defined (STACK_DYNAMIC_OFFSET) || defined (STACK_POINTER_OFFSET) || ! defined (STACK_BOUNDARY)
+#if defined (STACK_DYNAMIC_OFFSET) || defined (STACK_POINTER_OFFSET) || ! defined (PREFERRED_STACK_BOUNDARY)
 #define MUST_ALIGN 1
 #else
-#define MUST_ALIGN (STACK_BOUNDARY < BIGGEST_ALIGNMENT)
+#define MUST_ALIGN (PREFERRED_STACK_BOUNDARY < BIGGEST_ALIGNMENT)
 #endif
 
   if (MUST_ALIGN)
@@ -1093,11 +1097,11 @@ allocate_dynamic_stack_space (size, target, known_align)
      way of knowing which systems have this problem.  So we avoid even
      momentarily mis-aligning the stack.  */
 
-#ifdef STACK_BOUNDARY
+#ifdef PREFERRED_STACK_BOUNDARY
   /* If we added a variable amount to SIZE,
      we can no longer assume it is aligned.  */
 #if !defined (SETJMP_VIA_SAVE_AREA)
-  if (MUST_ALIGN || known_align % STACK_BOUNDARY != 0)
+  if (MUST_ALIGN || known_align % PREFERRED_STACK_BOUNDARY != 0)
 #endif
     size = round_push (size);
 #endif
diff --git a/function.c b/function.c
index 17b70e5..8d40633 100644
--- a/function.c
+++ b/function.c
@@ -132,6 +132,13 @@ int current_function_has_nonlocal_goto;
 
 int current_function_contains_functions;
 
+/* Nonzero if function being compiled doesn't contain any calls
+   (ignoring the prologue and epilogue).  This is set prior to
+   local register allocation and is valid for the remaining
+   compiler passes. */
+
+int current_function_is_leaf;
+
 /* Nonzero if the current function is a thunk (a lightweight function that
    just adjusts one of its arguments and forwards to another function), so
    we should try to cut corners where we can.  */
diff --git a/output.h b/output.h
index f1725b0..c92a4f4 100644
--- a/output.h
+++ b/output.h
@@ -360,6 +360,13 @@ extern int current_function_has_nonlocal_label;
 
 extern int current_function_contains_functions;
 
+/* Nonzero if function being compiled doesn't contain any calls
+   (ignoring the prologue and epilogue).  This is set prior to
+   local register allocation and is valid for the remaining
+   compiler passes. */
+
+extern int current_function_is_leaf;
+
 /* Nonzero if the current function returns a pointer type */
 
 extern int current_function_returns_pointer;
diff --git a/toplev.c b/toplev.c
index 45831c0..d411443 100644
--- a/toplev.c
+++ b/toplev.c
@@ -3470,6 +3470,11 @@ rest_of_compilation (decl)
      when it's safe to kill spill regs.  */
   max_label_num_after_reload = max_label_num ();
 
+  /* Determine if the current function is a leaf before running reload
+     since this can impact optimizations done by the prologue and
+     epilogue thus changing register elimination offsets.  */
+  current_function_is_leaf = leaf_function_p ();
+
   /* Unless we did stupid register allocation,
      allocate remaining pseudo-regs, then do the reload pass
      fixing up any insns that are invalid.  */

diff --git a/gcc/optabs.c b/gcc/optabs.c
index ba4b9ad1b..911907c08 100644
--- a/gcc/optabs.c
+++ b/gcc/optabs.c
@@ -6341,12 +6341,20 @@ expand_atomic_load (rtx target, rtx mem, enum memmodel model)
   if (icode != CODE_FOR_nothing)
     {
       struct expand_operand ops[3];
+      rtx_insn *last = get_last_insn ();
+      if (is_mm_seq_cst (model))
+	expand_asm_memory_barrier ();
 
       create_output_operand (&ops[0], target, mode);
       create_fixed_operand (&ops[1], mem);
       create_integer_operand (&ops[2], model);
       if (maybe_expand_insn (icode, 3, ops))
-	return ops[0].value;
+	{
+	  if (!is_mm_relaxed (model))
+	    expand_asm_memory_barrier ();
+	  return ops[0].value;
+	}
+      delete_insns_since (last);
     }
 
   /* If the size of the object is greater than word size on this target,
@@ -6391,11 +6399,19 @@ expand_atomic_store (rtx mem, rtx val, enum memmodel model, bool use_release)
   icode = direct_optab_handler (atomic_store_optab, mode);
   if (icode != CODE_FOR_nothing)
     {
+      rtx_insn *last = get_last_insn ();
+      if (!is_mm_relaxed (model))
+	expand_asm_memory_barrier ();
       create_fixed_operand (&ops[0], mem);
       create_input_operand (&ops[1], val, mode);
       create_integer_operand (&ops[2], model);
       if (maybe_expand_insn (icode, 3, ops))
-	return const0_rtx;
+	{
+	  if (is_mm_seq_cst (model))
+	    expand_asm_memory_barrier ();
+	  return const0_rtx;
+	}
+      delete_insns_since (last);
     }
 
   /* If using __sync_lock_release is a viable alternative, try it.
diff --git a/gcc/testsuite/gcc.dg/atomic/pr80640-2.c b/gcc/testsuite/gcc.dg/atomic/pr80640-2.c
new file mode 100644
index 000000000..a73505409
--- /dev/null
+++ b/gcc/testsuite/gcc.dg/atomic/pr80640-2.c
@@ -0,0 +1,32 @@
+/* { dg-do run } */
+/* { dg-options "-pthread" } */
+/* { dg-require-effective-target pthread } */
+
+#include <pthread.h>
+
+static volatile int sem1;
+static _Atomic  int sem2;
+
+static void *f(void *va)
+{
+  void **p = va;
+  if (*p) return *p;
+  sem1 = 1;
+  while (!__atomic_load_n(&sem2, __ATOMIC_ACQUIRE));
+  // GCC used to RTL-CSE this and the first load, causing 0 to be returned
+  return *p;
+}
+
+int main()
+{
+  void *p = 0;
+  pthread_t thr;
+  if (pthread_create(&thr, 0, f, &p))
+    return 2;
+  while (!sem1);
+  __atomic_thread_fence(__ATOMIC_ACQUIRE);
+  p = &p;
+  __atomic_store_n(&sem2, 1, __ATOMIC_RELEASE);
+  pthread_join(thr, &p);
+  return !p;
+}
diff --git a/gcc/testsuite/gcc.dg/atomic/pr81316.c b/gcc/testsuite/gcc.dg/atomic/pr81316.c
new file mode 100644
index 000000000..ef1009571
--- /dev/null
+++ b/gcc/testsuite/gcc.dg/atomic/pr81316.c
@@ -0,0 +1,29 @@
+/* { dg-do run } */
+/* { dg-options "-pthread" } */
+/* { dg-require-effective-target pthread } */
+
+#include <pthread.h>
+#include <stdlib.h>
+
+static _Atomic int sem1;
+
+static void *f(void *va)
+{
+  void **p = va;
+  while (!__atomic_load_n(&sem1, __ATOMIC_ACQUIRE));
+  exit(!*p);
+}
+
+int main(int argc)
+{
+  void *p = 0;
+  pthread_t thr;
+  if (pthread_create(&thr, 0, f, &p))
+    return 2;
+  // GCC used to RTL-DSE this store
+  p = &p;
+  __atomic_store_n(&sem1, 1, __ATOMIC_RELEASE);
+  int r = -1;
+  while (r < 0) asm("":"+r"(r));
+  return r;
+}
diff --git a/gcc/testsuite/gcc.target/arm/stl-cond.c b/gcc/testsuite/gcc.target/arm/stl-cond.c
index de14bb580..e47ca6bf3 100644
--- a/gcc/testsuite/gcc.target/arm/stl-cond.c
+++ b/gcc/testsuite/gcc.target/arm/stl-cond.c
@@ -1,19 +1,61 @@
-/* { dg-do compile } */
-/* { dg-require-effective-target arm_arm_ok } */ 
+/* { dg-do compile { target arm*-*-* } } */
+/* { dg-require-effective-target arm_arm_ok } */
 /* { dg-require-effective-target arm_arch_v8a_ok } */
 /* { dg-options "-O2 -marm" } */
 /* { dg-add-options arm_arch_v8a } */
 
-struct backtrace_state
-{
-  int threaded;
-  int lock_alloc;
-};
+/* We want to test that the STL instruction gets the conditional
+   suffix when under a COND_EXEC.  However, COND_EXEC is very hard to
+   generate from C code because the atomic_store expansion adds a compiler
+   barrier before the insn, preventing if-conversion.  So test the output
+   here with a hand-crafted COND_EXEC wrapped around an STL.  */
 
-void foo (struct backtrace_state *state)
+void __RTL (startwith ("final")) foo (int *a, int b)
 {
-  if (state->threaded)
-    __sync_lock_release (&state->lock_alloc);
+(function "foo"
+  (param "a"
+    (DECL_RTL (reg/v:SI r0))
+    (DECL_RTL_INCOMING (reg:SI r0))
+  )
+  (param "b"
+    (DECL_RTL (reg/v:SI r1))
+    (DECL_RTL_INCOMING (reg:SI r1))
+  )
+  (insn-chain
+    (block 2
+	(edge-from entry (flags "FALLTHRU"))
+	(cnote 5 [bb 2] NOTE_INSN_BASIC_BLOCK)
+
+  (insn:TI 7 (parallel [
+	(set (reg:CC cc)
+	     (compare:CC (reg:SI r1)
+			 (const_int 0)))
+	(set (reg/v:SI r1)
+	     (reg:SI r1 ))
+        ])  ;; {*movsi_compare0}
+     (nil))
+
+  ;; A conditional atomic store-release: STLNE for Armv8-A.
+  (insn 10 (cond_exec (ne (reg:CC cc)
+	   (const_int 0))
+	(set (mem/v:SI (reg/v/f:SI r0) [-1  S4 A32])
+		(unspec_volatile:SI [
+		(reg/v:SI r1)
+		(const_int 3)
+		] VUNSPEC_STL))) ;; {*p atomic_storesi}
+	(expr_list:REG_DEAD (reg:CC cc)
+	(expr_list:REG_DEAD (reg/v:SI r1)
+	(expr_list:REG_DEAD (reg/v/f:SI r0)
+		(nil)))))
+      (edge-to exit (flags "FALLTHRU"))
+    ) ;; block 2
+  ) ;; insn-chain
+  (crtl
+    (return_rtx
+      (reg/i:SI r0)
+    ) ;; return_rtx
+  ) ;; crtl
+) ;; function
 }
 
 /* { dg-final { scan-assembler "stlne" } } */
